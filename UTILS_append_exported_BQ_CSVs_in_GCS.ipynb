{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e34784-c18d-49ea-ac30-1fd4d20ba77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a0c7503-32a6-4bb8-b93f-0b67d3eee70a",
   "metadata": {},
   "source": [
    "# Append all data files exported from BigQuery to CSVs in GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d492e91b-6a8e-4e6e-9e27-603064b1a2f4",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "* label is the first column in the exported files\n",
    "* all other columns can be labeled x_{n} where number is the position of the column in relation to the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544fa65-e181-480a-a719-a9bd34ccbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# extract table from from BQ to GCS CSVs\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03118164-de22-4b11-896d-d524c8dd699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "P = ! gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = P[0]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63158182-b56e-4228-8465-8ce35d4dfc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET = \"ds_central1\"\n",
    "BQ_TABLE = \"tab_class_732inps_1600krows_tra\"\n",
    "TRAINING_DATA_BUCKET = \"ap-alto-ml-1000-bucket-us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4dbf2784-339a-43c6-b141-e6d80e8b2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_rbddeba452ee2275_00000185a8933615_1 ... (108s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "! bq extract \\\n",
    "  --destination_format=CSV \\\n",
    "  --field_delimiter=',' \\\n",
    "  --print_header=false \\\n",
    "  {BQ_DATASET}.{BQ_TABLE} \\\n",
    "  gs://{TRAINING_DATA_BUCKET}/{BQ_TABLE}_*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6348a80-b33d-40e0-a4a4-54395fed88f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT count(*) FROM `ap-alto-ml-1000.ds_central1.INFORMATION_SCHEMA.COLUMNS` WHERE table_name = 'tab_class_732inps_1600krows_tra'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "733"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_col_count(PROJECT_ID, REGION, BQ_DATASET, BQ_TABLE):\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(location=REGION, project=PROJECT_ID)    \n",
    "    query = f\"SELECT count(*) FROM `{PROJECT_ID}.{BQ_DATASET}.INFORMATION_SCHEMA.COLUMNS` WHERE table_name = '{BQ_TABLE}'\"\n",
    "    print(query)\n",
    "    query_job = client.query(query, location=REGION, )\n",
    "    return query_job.to_dataframe().values.tolist()[0][0]\n",
    "\n",
    "bq_table_column_count = get_col_count(PROJECT_ID, REGION, BQ_DATASET, BQ_TABLE)\n",
    "bq_table_column_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a8bd8-a833-4aa2-92b7-08f1a82ffe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# combine CSVs into a single file\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82b0019-68c1-465c-98a9-707be3df179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380641d-0bdf-4e8f-aaee-4c7de30681ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file containing column headers\n",
    "l = list()\n",
    "l.append(\"label\")\n",
    "for i in range(bq_table_column_count):\n",
    "    l.append(f\"x_{i}\")\n",
    "\n",
    "\n",
    "file_with_headers = f\"{BQ_TABLE}_headers.csv\"\n",
    "with open(file_with_headers, 'w', newline='') as csvfile:\n",
    "    w = csv.writer(csvfile, delimiter=',')\n",
    "    w.writerow(l)\n",
    "    \n",
    "# copy file to GCS dir with data\n",
    "! gsutil cp {file_with_headers} gs://{TRAINING_DATA_BUCKET}/{file_with_headers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "443586ef-d493-4119-8e49-02ee7eb6cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source files for GCS compose\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(TRAINING_DATA_BUCKET)\n",
    "\n",
    "sources = list()\n",
    "sources.append(bucket.get_blob(file_with_headers))\n",
    "\n",
    "# get blob names from GCS\n",
    "blobs = bucket.list_blobs()\n",
    "for b in bucket.list_blobs():\n",
    "    if b.name != file_with_headers:\n",
    "        sources.append(bucket.get_blob(b.name))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# chunk for GCS compose - max num files allowed at one time is 32\n",
    "n = 30\n",
    "source_lists = [sources[i:i + n] for i in range(0, len(sources), n)]\n",
    "\n",
    "if len(source_lists) == 1:\n",
    "    destination_blob_name = f\"{BQ_TABLE}.csv\"\n",
    "    destination = bucket.blob(destination_blob_name)\n",
    "    destination.content_type = \"text/plain\"\n",
    "    destination.compose(source_lists[0])\n",
    "else:\n",
    "    for idx, source_list in enumerate(source_lists):\n",
    "        if idx==0 and idx < len(source_lists)-1:\n",
    "            destination_blob_name = f\"{BQ_TABLE}_temp_{idx}.csv\"\n",
    "            destination = bucket.blob(destination_blob_name)\n",
    "            destination.content_type = \"text/plain\"\n",
    "            destination.compose(source_list)\n",
    "        if idx > 0 and idx < len(source_lists)-1:\n",
    "            last_temp = [bucket.get_blob(destination.name)]\n",
    "            destination_blob_name = f\"{BQ_TABLE}_temp_{idx}.csv\"\n",
    "            destination = bucket.blob(destination_blob_name)\n",
    "            destination.content_type = \"text/plain\"\n",
    "            destination.compose( last_temp + source_list)\n",
    "        elif idx == len(source_lists)-1:\n",
    "            last_temp = [bucket.get_blob(destination.name)]\n",
    "            destination_blob_name = f\"{BQ_TABLE}.csv\"\n",
    "            destination = bucket.blob(destination_blob_name)\n",
    "            destination.content_type = \"text/plain\"\n",
    "            destination.compose( last_temp + source_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c112057-a0fd-4488-a901-4ce2e0b959b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_TRAIN_URI = f\"gs://{TRAINING_DATA_BUCKET}/{BQ_TABLE}.csv\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
